\documentclass[11pt, letterpaper, onecolumn]{article}

\usepackage[english]{babel}
\usepackage{soul}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage[german=quotes]{csquotes}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{gensymb}
\usepackage{units}
\usepackage{hhline}
\usepackage{color}
\usepackage{titling}
\usepackage[normalem]{ulem}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{pgfplots}
\usepackage{array}
\usepackage{makecell}
\usepackage{subfigure}
\usepackage{lipsum}
\usepackage{url}
\usepackage{relsize}

\newgeometry{a4paper, left=20mm, right=20mm, top=30mm, bottom=30mm}
\definecolor{pantone294}{cmyk}{1,0.6,0,0.2}
\setlength{\columnsep}{6mm} 

\title{Project 1: QM point particle in external potential} 
\author{Florian Telleis / Florian Hollants / Mickey Wilke}
\date{\today}

\pagestyle{fancy}
\lfoot{Humboldt-Universit√§t zu Berlin}
\rfoot{Project 1.2} 



\begin{document}
	
		Deckblatt: Revision 1.1 + Sub-project 1.2 
		
		\newpage
	
	
	
	
	
	
	\tableofcontents
	
	
	
	
	
	\vspace{0.5cm}
	
	
	\section{What changed regarding our first submission}
	
	\subsection{Modularity}
	As a first step, we improved our file management by implementing a modularity of files, dividing our prior one python file into seven (and of course more files for the second sub-project). The content of these modules did not change if not stated otherwise in the sections Variables, Tests or Convergence. \\
	The new file system is listed below: \\ 
	- "variables.py": defining variables and few general functions\\
	- "hamiltonian.py": defining potential and hamiltonian\\
	- "test$\_$hamiltonian.py": defining the tests for the hamiltonian and also running them for multiple $N$ and dimensions \\
	- "integrators.py": defining both the integrators \\
	- "test$\_$integrators.py": defining the tests for the integrators and also running them for multiple $N$ and dimensions \\
	- "test$\_$integr$\_$converg.py": testing the dependence of the integrators on $M$ or rather $\tau$ \\
	- "animation.py": defining functions for animation and animating


\subsection{Variables}
	We then changed/deleted all our old variables depending on other parameters to be now only defined by dimensionless values, without changing their value in regards to our first submission.
	In our case of modularity where we work with the same variables in multiple python files it is also practical to add an extra python file for storing our constant variables (and some basic functions used in multiple other files). We can then import these global variables by "from variables import *". \\
	This change worked with most of the prior functions and tests except when needing to change certain variables of a function in a separate file from where the function is defined. Explicitly, what changed: \\
	\textbf{1.} For the new hamiltonian tests (see 1.3), we had to change our definition of the potential to take $N$ from the size of the given array instead of using the global $N$.\\
	\textbf{2.} A second change regarding the hamiltonian file was done. We defined potential$\_$variable and \\hamilton$\_$variable which are the same as our old potential and hamiltonian but take $\mu$ and $\epsilon$ as an input for the convergence of eigenvalues/-vectors in the second sub-project. \\
	\textbf{3.} Similarly, but for the first sub-project, for testing the integrators in dependence of $M$ or rather $\tau$, we had to add an additional input for $\tau$ into the integrators (and therefore the tests). Therefore, the dependence could be evaluated without having to define the integrators in the same file as the tests, which was done in our first submission.


	\subsection{Tests}
	The tests we included in the sub-project 1 were in general not modified for this revision. We now ran these tests of the hamiltonian and integrators for multiple dimensions and $N$ for proper testing. Here we take focus on $N$ = 5, 10, 15 and 20; because the relevance of boundary conditions and resulting errors decrease for large $N$. The exact output for running the testing files is shown in the following figures for each of the $N$ for 4 dimensions (hamiltonian) and 3 dimensions (integrators, due to long run-time). The tests are including "randomly" generated arrays. Therefore these tests run 10 times each and depending on the criteria the maximum error of each test is saved.\\
	Also notice that the output shows $N$ as a float, it is an integer nonetheless.
	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=15cm]{"test_hamiltonian2.png"}
	\caption{Output of our test functions for the hamiltonian for multiple $N$ and $D$; 10 iterations.}
	\end{center}
	\end{figure}
	\\
	One can see that the errors for testing the linearity and eigenvectors are in the range of $10^{-14}$ to $10^{-13}$, where for higher dimensions the error increases. It also seems that for higher $N$ the errors also increase. But in some cases this could also be due to the "random" generation. We observe the same dependence for the hermiticity with errors up to $10^{-10}$ and eigenvectors with error up to $10^{-12}$. The positivity of the hamiltonian was correct all the iterations.\\
	\\
	Now our integrator tests. 
	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=13cm]{"test_integrators2.png"}
	\caption{Output of our test functions for the integrators for multiple $N$ and $D$; 10 iterations.}
	\end{center}
	\end{figure}
	\\
	The errors of unitarity for the strang-splitting integrator and linearity for both of the integrators are in the range of $10^{-16}-10^{-15}$, which is even smaller than for the hamiltonian tests and checks with the expectation. The unitarity errors for the second-order integrator are far larger even in 1D with $10^{-5}$ and rising for higher dimensions. Even greater errors can be seen for the energy conservation tests. This is expected as energy conservation is not given for these integrators. The important detail however, is that no significant lowering of errors for higher $N$ is seen, which is most likely due $N=20$ still being relatively small and using randomly generated arrays, which change the error maxima.
	\\
	\\
	Below, all the tests were run for one iteration for $N=5$ and $D=9$ as an additional testing. One can see the same behaviour as above, just exaggerated. \\
	With all these evaluated results we can safely conclude that our tests run (correctly) in more dimensions than $D=1$, even for small $N$.
	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=6cm]{"test_hamiltonian-9D2.png"}
	\caption{Output of our test functions for the hamiltonian for $N=5$ and $D=9$; 1 iteration.}
	\end{center}
	\end{figure}
	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=7cm]{"test_integrators-9D2.png"}
	\caption{Output of our test functions for the integrators for $N=5$ and $D=9$; 1 iteration.}
	\end{center}
	\end{figure}
	
	
	
	
	
	
	
	\subsection{Convergence with rel() function xxxxxxxchange title}
	We are interested in seeing how certain quantities behave for different values of $M$ and therefore different values of $\tau$. We will look at $\Delta E$ for both integrators, as well as the deviation of the norm for the second-order integrator, and lastly, we will look at the average difference between the two integrators. For that we will take a specific time $T$ and calculate the time evolution of one specific state in $M$ steps up to that time $T$. Afterwards, we repeat the same process for different values of $M$.\\
    Since both integrators use approximations that improve with smaller values of $\tau$, we expect the deviation of the norm and energy to decrease with increasing $M$. Lets look at the second order integrator first: $U(t,0)=1-i\hat{H}\hat{\tau}-\frac12\hat{H}^2\hat{\tau}^2$
    \begin{align*}
        \braket{\psi_t|\psi_t}&=\bra{\psi_0}[U^\dagger(t,0)U(t,0)]^M\ket{\psi_0}\\
        &=\bra{\psi_0}\left[1+\frac{1}{4}\hat{H}^4\hat{\tau}^4 \right]^M\ket{\psi_0}\\
        &=\bra{\psi_0}1+\frac{M}{4}\hat{H}^4\hat{\tau}^4+\mathcal{O}(\hat{\tau}^6)\ket{\psi_0}\\
        &\approx \braket{\psi_0|\psi_0}+\frac{\hat{T}\hat{E}_0^4}{4}\hat{\tau}^3\braket{\psi_0|\psi_0}\\
        \implies \Delta|\psi_t|^2&\sim\hat{\tau}^3\sim M^{-3}
    \end{align*}
    Now we look at the strang-splitting integrator: $U(t,0)=e^{-i\frac{\hat{V}\hat{\tau}}{2}}e^{-i\hat{K}\hat{\tau}}e^{-i\frac{\hat{V}\hat{\tau}}{2}}$
    \begin{align*}
        U(t,0)&=e^{-i\frac{\hat{V}\hat{\hat{\tau}}}{2}}e^{-i\hat{K}\hat{\hat{\tau}}}e^{-i\frac{\hat{V}\hat{\hat{\tau}}}{2}}\\
        &=(1-i\hat{\tau}\frac{V}{2}-\frac{1}{2}\left(\hat{\tau}\frac{V}{2}\right)^2+\frac{i}{6}\left(\hat{\tau}\frac{V}{2}\right)^3)(1-i\hat{\tau} K-\frac{1}{2}\left(\hat{\tau} K\right)^2+\frac{i}{6}\left(\hat{\tau} K\right)^3)(1-i\hat{\tau}\frac{V}{2}-\frac{1}{2}\left(\hat{\tau}\frac{V}{2}\right)^2+\frac{i}{6}\left(\hat{\tau}\frac{V}{2}\right)^3)\\&+\mathcal(O)(\hat{\tau}^4)\\
        &=1-i\hat{\tau} H-\frac{1}{2}\hat{\tau}^2(K^2+V^2+KV+VK)\\&+\frac{i\hat{\tau}^3}{6}\left(8\left(\frac{V}{2} \right)^3 + 3K\left(\frac{V}{2} +\right)^2 + 3\left(\frac{V}{2} \right)^2K +3K^2\frac{V}{2}+3\frac{V}{2}K^2 +6\frac{V}{2}K\frac{V}{2}+K^3 \right) +\mathcal{O}(\hat{\tau}^4)\\
        &=e^{-iH\hat{\tau}}+\mathcal{O}(\hat{\tau}^3)
    \end{align*}
Therefore we can conclude that
\begin{align*}
    \braket{\psi_t|\psi_t}&=\bra{\psi_0}U^\dagger U\ket{\psi_0}\\
    &=\bra{\psi_0}((e^{iH\hat{\tau}}+\mathcal{O}(\hat{\tau}^3))(e^{-iH\hat{\tau}}+\mathcal{O}(\hat{\tau}^3)))^M\ket{\psi_0}\\
    &=\bra{\psi_0}(1+\mathcal{O}(\hat{\tau}^3))^M\ket{\psi_0}\\
    &=\braket{\psi_0|\psi_0}+\bra{\psi_0}M\mathcal{O}(\hat{\tau}^3)\ket{\psi_0}\\
    &=\braket{\psi_0|\psi_0}+\bra{\psi_0}\mathcal{O}(\hat{\tau}^2)\ket{\psi_0}
\end{align*}
We therefore expect $\Delta|\psi_t|^2\sim\hat{\hat{\tau}}^2\sim M^{-2}$
	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=7cm]{"inf_vol_lim.png"}
	\caption{log-log plots against M}
	\end{center}
	\end{figure}
	The calculated are [[-3.0198987799444197, -1.9999479182661908], [-3.009455253603387, -2.004740059443111]], which aligns with our predictions.
	

	\\
 	\\
  	\\
   	\\
 	(noch die neue Animation reinpacken um zu zeigen dass modular funktioniert?)
  	\\
   	\\
    	\\
	
	\section{How the code works for the second sub-project}	
	- kurze Einf√ºhrung noch, dass das hier jetzt second sub-project
	
	
	
	\section{Workflow}
	- f√ºr Textaufbau ungef√§hr an workflow vom ersten Projekt halten, da hatte er ja nichts bem√§ngelt eigentlich
	
	
	
	\section{Implementing conjugate-gradient and power method}
	
	
	
	
	\section{Testing}
	!!! describe this time with detail and test all possible configurations!!!
	
	
	
	\section{Convergence of eigenvalues/eigenvectors}

 	\subsection{Infinite Volume Limit}
  	We first try to do the infinite volume limit, by using the arnoldi-method for a random starting vector and a fixed value of $\mu=20$, $\varepsilon=\frac{1}{60}$ and $N=40$. Then we repeat that process for bigger values of $N$ and plot the eigenvalues against $N$.
    	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=7cm]{"inf_vol_lim.png"}
	\caption{Eigenvalues of the of the Hamiltonian for different values of $N$}
	\end{center}
	\end{figure}
	The eigenvectors seem to have converged from $N=200$ onwards although two of the four eigenvalues seem to have vanished. Zooming in on one the lower dot reveals, that the two eigenvalues are much closer compared to the distance to the other two eigenvalues. Since we don't expect the eigenvalues to change much anymore, we say that the infinite volume limit is well approximated for $N>240$. 
 	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=7cm]{"eigenvectors_noParity.png"}
	\caption{Eigenvectors for $N=240$ with $\mu$ and $\varepsilon$ from the infinite volume series, plotted against $n\varepsilon=\frac{x}{r}$}
	\end{center}
	\end{figure}
 	First we notice, that the eigenvectors are exclusively real. This is, because we have chosen a real starting vector for the arnoldi method, which in return only produces real eigenvectors, since the hamiltonian has no imaginary part. We could have started with a complex starting vector, and gotten complex eigenvectors as well, but since we can multiply any eigenvector by a complex phase without changing the fact that it is still an eigenvector of the operator, we would have been able to generate the purely real eigenvectors from there as well.
	Secondly we notice, that while the lower two eigenvectors look symmetric/antisymmetric the same is not really true for the other two. That observation gives motivation to the next subsection in which we try to improve on the process by making use of the symmetries of our system.
	\subsection{Parity}
 	The Parity operator takes any function $\psi(x)$ from the hilbert space and returns $\mathbb{P}\psi(x)=\psi(-x)$. The operator has the two degenerate eigenvalues $\pm1$ which correspond to symmetric (even) and antisymmetric (odd) functions. Using the symmetry of our potential $V(x)=V(-x)$ we can show, that the parity operator commutes with the hamiltonian.

  	\begin{align*}
		[\hat{H},\hat{\mathbb{P}}]f(x)&=\hat{H}f(-x)-\hat{\mathbb{P}}\left(-\frac{\hbar^2}{2ma^2}\sum\limits_k\left(f(x+ae_k)+f(x-ae_k)-2f(x)\right)+V(x)f(x) \right)\\
	    &=-\frac{\hbar^2}{2ma^2}\sum\limits_k\left(f(-x+ae_k)+f(-x-ae_k)-2f(-x)\right)+V(x)f(-x)\\&+\frac{\hbar^2}{2ma^2}\sum\limits_k\left(f(-x+ae_k)+f(-x-ae_k)-2f(-x)\right)-V(-x)f(-x)\\
	    &=0
	\end{align*}
 	From this result follows, that we can simultaneously diagonalize both operators, meaning that any eigenvector of one operator has to be an eigenvector of the other operator as well. Being an eigenvector of the parity operator means, that $\psi(x)=\pm\psi(-x)$, the eigenvectors have to be either even or odd! This confirms what could have been presumed when looking at the results earlier.
	Now we want to use this result to improve the arnoldi method. To do this, we make the observation that applying the hamiltonian operator to an even/odd function will return an even/odd function. This can be shown by doing the explicit calculation, or by reasoning about the fact that an even/odd function is automatically an eigenvector of the parity operator. We want to show that $f(x)=\pm f(-x)\implies (\hat{H}f)(x)=\pm(\hat{H}f)(-x)$:
 	\begin{align*}
    		(\hat{H}f)(-x)&=\hat{\mathbb{P}}(\hat{H}f)(x)=\hat{H}(\hat{\mathbb{P}}f)(x)=\pm(\hat{H}f)(x)
	\end{align*}
 	Since the arnoldi-method consists of repeatedly applying the hamiltonian operator we can see, that using an even/odd vector as a starting guess ensures that we only find even/odd solutions, reducing the space of possible eigenvectors drastically, and since we know all eigenvectors of the hamiltonian must be even/odd we can be sure, that we aren't missing any. Using even/odd starting guesses we generated the eigenvectors for the lowest two eigenvalues for each starting guess, and plotted them all again.
  	\begin{figure} [h] 
	\begin{center}
	\includegraphics[width=7cm]{"eigenvectors_Parity.png"}
	\caption{Eigenvectors for $N=241$ with $\mu$ and $\varepsilon$ from the infinite volume series, plotted against $n\varepsilon=\frac{x}{r}$ using parity}
	\end{center}
	\end{figure}
 	We chose $N=241$ since it is easier to produce even and odd starting guesses for odd values of $N$. Not only do we see a lot of improvement especially for the higher two eigenvectors, the method is also a lot faster than before, making our code much more efficient.
	\subsection{Continuum Limit}
    	We want to look at the continuum limit in the infinite volume case. To do that we have to take the limit for $a\rightarrow0$. We have two variables that depend on $a$: $N=\frac{L}{a}$ and $\epsilon=\frac{a}{r}$. Since we only work with the dimensionless variables, we have to take the limit for $N\rightarrow\infty$ and $\epsilon\rightarrow0$ in such a way, that $N\epsilon=\frac{L}{r}=const$. To be in the infinite volume limit we will use $N\epsilon=\frac{241}{60}$. We will then make a list of multipliers $m\geq1$ and use values for $N=241*m$ and $\epsilon=\frac{1}{60\cdot m}$, we then plot the Eigenvalues against the different multipliers.







%	\hyperref[Quellen]{$^{[1]}$}	



	
%		\begin{figure} [h] 
%	\begin{center}
%	\subfigure[CGM]{\includegraphics[width=0.45\textwidth]{799px-Constant_current.svg.png}}
%    \subfigure[CHM]{\includegraphics[width=0.45\textwidth]{799px-Constant_height.svg.png}}
%\caption{Verschiedene Modi f√ºr das RTM Abrastern, entnommen aus \hyperref[Quellen]{[3]}}
%	\end{center}
%	\end{figure}


	

	
	%	\begin{figure} [h] 
%	\begin{center}
%	\includegraphics[width=8.8cm]{"QBER(R).jpg"}
%	\caption{QBER($R_{det}$) for three different photon sources and different attenuations; the data points are linearly connected for better visibility}
%	\end{center}
%	\end{figure}
	
	
	

%	\hyperref[Quellen]{$^{[1]}$}	

	
	
\newpage
	

	
%\section{Sources and Literature} \label{sources}
%
%		$[1]$ \textit{Titel} - Autor; (Vers.) Datum
%\vspace{0.4cm}
%		 \\
%		$[2]$ Internetseite: \textit{Titel} \\ \url{Link} \\- zuletzt besucht am Datum um Zeit
%		\vspace{0.4cm}
%		 \\
		
		
%		$[1]$ \textit{Titel} - Autor; (Vers.) Datum
%\vspace{0.4cm}
%		 \\
%		$[2]$ Internetseite: \textit{Titel} \\ \url{Link} \\- zuletzt besucht am Datum um Zeit
%		\vspace{0.4cm}
%		 \\

	
	

	
\section{Appendix} \label{sec:appendix}

%	\begin{figure}[h]	
%	\begin{center}	
%	\subfigure[Random input wavefunction]{\includegraphics[width=0.35\textwidth]{function1_ABS.png}}
%    \subfigure[Potential]
%    {\includegraphics[width=0.35\textwidth]{potential2.png}}
%    \subfigure[Second-order integration]
%    {\includegraphics[width=0.35\textwidth]{so-integrator1_ABS.png}}
%	\caption{Tests with (a) random 2D wavefunction, (b) resulting potential and (c) the following time evolution with the second-order integrator after a certain amount of time steps M (not known anymore, but around 2000 with T=2)}
%	\label{fig:2D-so_integr}
%	\end{center} 
%	\end{figure}
	
	




	
\end{document}
